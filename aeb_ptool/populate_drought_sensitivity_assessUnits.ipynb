{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arcpy\n",
    "import pandas as pd\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayer\n",
    "from arcgis.features import GeoAccessor, GeoSeriesAccessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb = r\"Q:\\projects\\Mwlrs\\AEB_Prioritization_Tool\\data\\AEB_Analysis.gdb\"\n",
    "arcpy.env.workspace= gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df of steams sensitivty\n",
    "fc= 'r20241209_streams_drought_sensitivity_ago'\n",
    "\n",
    "fields = [\"blue_line_key\", \"gnis_name\", \"assessment_unit_name\",\"watershed_group_id\", \"stream_sntvty_summary\"]\n",
    "\n",
    "data = []\n",
    "\n",
    "rowcount= 0\n",
    "with arcpy.da.SearchCursor(fc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        blue_key= row[0]\n",
    "        name = row[1]\n",
    "        unit = row[2]\n",
    "        wgrp = row[3]\n",
    "        snst = row[4]\n",
    "        data.append([blue_key, name, unit, wgrp, snst])\n",
    "\n",
    "        rowcount += 1\n",
    "\n",
    "df_strm = pd.DataFrame(data, columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df of assesement units\n",
    "fc= 'r20250117_AEB_Disturbance_Feature_Class'\n",
    "\n",
    "fields = [\"ASSESSMENT_UNIT_SOURCE_ID\", \"ASSESSMENT_UNIT_NAME\", 'WATERSHED_GROUP_ID']\n",
    "\n",
    "data = []\n",
    "\n",
    "rowcount= 0\n",
    "with arcpy.da.SearchCursor(fc, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        id= row[0]\n",
    "        name = row[1]\n",
    "        wgrp = row[2]\n",
    "        data.append([id, name, wgrp])\n",
    "\n",
    "        rowcount += 1\n",
    "\n",
    "df_asun = pd.DataFrame(data, columns=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df of assesement units/stream intersection (for unnamed Assesment Units)\n",
    "fcINT= 'r20250117_tempo_assesUnit_stream_intersect'\n",
    "\n",
    "fieldsNA = [\"ASSESSMENT_UNIT_SOURCE_ID\", \"stream_sntvty_summary\"]\n",
    "\n",
    "dataNA = []\n",
    "\n",
    "rowcount= 0\n",
    "with arcpy.da.SearchCursor(fcINT, fieldsNA) as cursor:\n",
    "    for row in cursor:\n",
    "        idNA= row[0]\n",
    "        snstNA = row[1]\n",
    "        dataNA.append([idNA, snstNA])\n",
    "\n",
    "        rowcount += 1\n",
    "\n",
    "df_asun_na = pd.DataFrame(dataNA, columns=fieldsNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Assesement units: assign sensitivity levels by matching them with the corresponding named streams.\n",
    "df_strm = df_strm[df_strm['assessment_unit_name'] != '']\n",
    "df_asun = df_asun[df_asun['ASSESSMENT_UNIT_NAME'] != 'N/A']\n",
    "\n",
    "df_strm_gr = df_strm.groupby([\"assessment_unit_name\", 'watershed_group_id' ,\"stream_sntvty_summary\"]).size().reset_index(name='count')\n",
    "\n",
    "# Mapping for ranks\n",
    "rank_mapping = {\n",
    "    \"Winter & Summer Sensitive\": 4,\n",
    "    \"Winter Sensitive\": 3,\n",
    "    \"Summer Sensitive\": 2,\n",
    "    \"Non-sensitive\": 1\n",
    "}\n",
    "\n",
    "# Adding the rank column\n",
    "df_strm_gr[\"rank\"] = df_strm_gr[\"stream_sntvty_summary\"].map(rank_mapping)\n",
    "\n",
    "df_asun_nm = df_strm_gr.loc[\n",
    "    df_strm_gr.groupby([\"assessment_unit_name\", \"watershed_group_id\"])[\"rank\"].idxmax()\n",
    "]\n",
    "\n",
    "df_asun_nm = df_asun_nm.reset_index(drop=True)\n",
    "\n",
    "# Fix the Watershed ID of Meldrum Creek\n",
    "df_asun_nm.loc[df_asun_nm['assessment_unit_name'] == 'Meldrum Creek', 'watershed_group_id'] = 215\n",
    "\n",
    "df_asun_nm.rename(columns={\"stream_sntvty_summary\": \"drought_snstvty\"}, inplace=True)\n",
    "df_asun_nm = df_asun_nm[['assessment_unit_name', 'watershed_group_id','drought_snstvty']]\n",
    "\n",
    "df_asun_nm = pd.merge(\n",
    "    df_asun,\n",
    "    df_asun_nm,\n",
    "    how= 'left',\n",
    "    left_on= ['ASSESSMENT_UNIT_NAME', 'WATERSHED_GROUP_ID'],\n",
    "    right_on= ['assessment_unit_name', 'watershed_group_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the unnamed assessment units: sensitivity levels them the highest sensitivity level of the streams within the assessment unit area\n",
    "df_asun_na[\"rank\"] = df_asun_na[\"stream_sntvty_summary\"].map(rank_mapping)\n",
    "\n",
    "df_asun_unn = df_asun_na.loc[df_asun_na.groupby(\"ASSESSMENT_UNIT_SOURCE_ID\")[\"rank\"].idxmax()]\n",
    "\n",
    "df_asun_unn = df_asun_unn.reset_index(drop=True)\n",
    "\n",
    "df_asun_unn.rename(columns={\"stream_sntvty_summary\": \"drought_snstvty\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the final df\n",
    "df= pd.concat(\n",
    "    [\n",
    "    df_asun_nm[['ASSESSMENT_UNIT_SOURCE_ID', 'drought_snstvty']],\n",
    "    df_asun_unn[['ASSESSMENT_UNIT_SOURCE_ID', 'drought_snstvty']]\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['ASSESSMENT_UNIT_SOURCE_ID'] = df['ASSESSMENT_UNIT_SOURCE_ID'].round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the FeatureClass with new Drought Sensitivity values\n",
    "fc= 'r20250117_AEB_Disturbance_Feature_Class'\n",
    "# Convert dataframe to dictionary for quick lookup\n",
    "drought_values = df.set_index('ASSESSMENT_UNIT_SOURCE_ID')['drought_snstvty'].to_dict()\n",
    "\n",
    "# Update the feature class using arcpy UpdateCursor\n",
    "with arcpy.da.UpdateCursor(fc, ['ASSESSMENT_UNIT_SOURCE_ID', 'Drought_Sensitivity']) as cursor:\n",
    "    for row in cursor:\n",
    "        unit_id = row[0]  # ASSESSMENT_UNIT_SOURCE_ID\n",
    "        if unit_id in drought_values:\n",
    "            row[1] = drought_values[unit_id]  # Update Drought_Sensitivity\n",
    "            cursor.updateRow(row)\n",
    "\n",
    "print(\"Feature class updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the AGOL layer with new Drought Sensitivity values\n",
    "AGO_HOST = 'https://governmentofbc.maps.arcgis.com'\n",
    "AGO_USERNAME_DSS = 'XXX'\n",
    "AGO_PASSWORD_DSS = 'XXX'\n",
    "gis= GIS(AGO_HOST, AGO_USERNAME_DSS, AGO_PASSWORD_DSS, verify_cert=False)\n",
    "\n",
    "agol_item_id = '7326558fb14c476bbfc8b7b6ddb45ada'\n",
    "\n",
    "feature_layer = gis.content.get(agol_item_id).layers[0]  # Assuming first layer in the item\n",
    "\n",
    "# Query all features in the layer\n",
    "print ('Reading records from AGOL layer')\n",
    "features = feature_layer.query(where=\"1=1\", out_fields=\"ASSESSMENT_UNIT_SOURCE_ID, Drought_Sensitivity\", return_geometry=False)\n",
    "features_dict = {f.attributes['ASSESSMENT_UNIT_SOURCE_ID']: f for f in features}\n",
    "\n",
    "# Update features with new drought sensitivity values\n",
    "print ('\\nUpdating data in batches')\n",
    "updated_features = []\n",
    "for index, row in df.iterrows():\n",
    "    unit_id = row['ASSESSMENT_UNIT_SOURCE_ID']\n",
    "    if unit_id in features_dict:\n",
    "        feature = features_dict[unit_id]\n",
    "        feature.attributes['Drought_Sensitivity'] = row['drought_snstvty']\n",
    "        updated_features.append(feature)\n",
    "\n",
    "# Function to batch updates\n",
    "def batch_update(features, batch_size=100):\n",
    "    for i in range(0, len(features), batch_size):\n",
    "        batch = features[i:i + batch_size]\n",
    "        response = feature_layer.edit_features(updates=batch)\n",
    "        print(f\"..batch {i // batch_size + 1}: Update response:\", response['updateResults'][0])\n",
    "\n",
    "# Batch and update\n",
    "if updated_features:\n",
    "    batch_update(updated_features, batch_size=100)  # Adjust batch size as needed\n",
    "else:\n",
    "    print(\"No features updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of fields and aliases from the Web map (for backup purpose)\n",
    "\n",
    "from arcgis.mapping import WebMap\n",
    "\n",
    "webmap_item = gis.content.get(\"0b56a5d091a04eacb3559cf33a53281f\")\n",
    "webmap = WebMap(webmap_item)\n",
    "\n",
    "# Recursive function to search for a layer in group layers\n",
    "def find_layer_by_name(layers, name):\n",
    "    for layer in layers:\n",
    "        if layer.get(\"title\") == name:\n",
    "            return layer\n",
    "        elif \"layers\" in layer:  # Check if it's a group layer\n",
    "            result = find_layer_by_name(layer[\"layers\"], name)\n",
    "            if result:\n",
    "                return result\n",
    "    return None\n",
    "\n",
    "# Find the specified feature layer by name\n",
    "layer_name = \"Current (2018) Aquatic Ecosystems Disturbance - All\"\n",
    "feature_layer = find_layer_by_name(webmap.layers, layer_name)\n",
    "\n",
    "if not feature_layer:\n",
    "    raise ValueError(f\"Layer with name '{layer_name}' not found in the Web Map.\")\n",
    "\n",
    "# Check if field aliases are defined in the Web Map\n",
    "if \"popupInfo\" in feature_layer:\n",
    "    fields = feature_layer[\"popupInfo\"][\"fieldInfos\"]\n",
    "    data = [{\"Field Name\": field[\"fieldName\"], \"Alias\": field.get(\"label\", field[\"fieldName\"])} for field in fields]\n",
    "else:\n",
    "    # Fall back to the fields in the Feature Layer URL\n",
    "    if \"url\" in feature_layer:\n",
    "        from arcgis.features import FeatureLayer\n",
    "        layer_url = feature_layer[\"url\"]\n",
    "        fl = FeatureLayer(layer_url)\n",
    "        fields = fl.properties.fields\n",
    "        data = [{\"Field Name\": field[\"name\"], \"Alias\": field.get(\"alias\", field[\"name\"])} for field in fields]\n",
    "    else:\n",
    "        raise ValueError(f\"The layer '{layer_name}' does not contain aliases in the Web Map or in the Feature Layer.\")\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
